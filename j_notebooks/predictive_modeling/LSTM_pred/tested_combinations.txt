With the dirty version of the week series (with year end bias):

model.add(keras.layers.LSTM(32, input_shape=(n_prev, 1), return_sequences=True))
model.add(keras.layers.Dropout(0.1))
model.add(keras.layers.LSTM(32, input_shape=(n_prev, 1), return_sequences=True))
model.add(keras.layers.Dropout(0.1))
model.add(keras.layers.LSTM(32, input_shape=(n_prev, 1), return_sequences=True))
model.add(keras.layers.Dropout(0.1))
model.add(keras.layers.Dense(1, activation='linear'))
model.compile(optimizer='rmsprop',loss='mean_squared_error')

model.fit(x_train, y_train, batch_size=32, epochs=1000)

with layer + dropout rates:
0.1, 0.1, 0.1: fl, mean
0,2, 0.1: fl, mean
0.21, 0.21: fl, mean 
0.2, 0.2, 0.2: fl, mean
(all flattened out at loss 1626)

success was with this:
model = keras.Sequential()
model.add(keras.layers.LSTM(32, input_shape=(n_prev, 1), return_sequences=True))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.LSTM(32, input_shape=(n_prev, 1), return_sequences=True))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.LSTM(32, return_sequences=False))
model.add(keras.layers.Dense(1, activation='linear'))
model.compile(optimizer='rmsprop',
              loss='mean_squared_error')
              
              
with 2000 epochs: loss=857


with 4 layers, all 0.2, ep=2000: 
loss=857

With the adjusted (true) weeks series:
window: n_prev=200
- 0.2, 0.2, ep=10000, b=32: loss=1065, flat
- 0.18 x4, ep= 2000, b=32: loss=1093, flat
- 0.2  x4, ep=10000, b=32: 1065, flat

- 0.21 x2, ep= 2000, b=32: loss=538, curve! 
--- but loss function fluctuates wildly in (at least) the last 1000 epochs. 
--- doesn't look promising
